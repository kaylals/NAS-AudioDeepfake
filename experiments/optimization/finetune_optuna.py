import optuna
import torch
import torch.nn as nn
import numpy as np
import logging
import sys
from pathlib import Path
from torch.optim.lr_scheduler import ReduceLROnPlateau
from tensorboardX import SummaryWriter

from models.model import Network
from func.functions import train_from_scratch, validate
from ASVDataloader.ASVRawDataset import ASVRawDataset
from utils.utils import Genotype

def objective(trial):
    # ===== è¶…å‚æ•°æœç´¢ç©ºé—´ =====
    lr = trial.suggest_float("lr", 1e-5, 1e-2, log=True)
    weight_decay = trial.suggest_float("weight_decay", 1e-5, 1e-2, log=True)
    batch_size = trial.suggest_categorical("batch_size", [8, 16, 32])
    class_weight_ratio = trial.suggest_float("class_weight_ratio", 1.0, 5.0)
    drop_path_prob = trial.suggest_float("drop_path_prob", 0.1, 0.4)

    # ===== å›ºå®šå‚æ•° =====
    pretrained_model = "pre_trained_models/4-16.pth"
    arch = "Genotype(normal=[('sep_conv_5x5', 0), ('sep_conv_5x5', 1), ('sep_conv_3x3', 2), ('dil_conv_5x5', 0), ('avg_pool_3x3', 2), ('max_pool_3x3', 3), ('avg_pool_3x3', 2), ('avg_pool_3x3', 4)], normal_concat=range(2, 6), reduce=[('sep_conv_5x5', 1), ('sep_conv_3x3', 0), ('dil_conv_3x3', 2), ('avg_pool_3x3', 0), ('dil_conv_5x5', 2), ('sep_conv_3x3', 3), ('dil_conv_3x3', 2), ('avg_pool_3x3', 4)], reduce_concat=range(2, 6))"
    data_path = Path("../../data/test_sample")

    # è®¾ç½®è®¾å¤‡å’Œéšæœºç§å­
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    torch.manual_seed(42)
    np.random.seed(42)

    print(f"ğŸš€ Trial {trial.number}: lr={lr:.2e}, wd={weight_decay:.2e}, bs={batch_size}, ratio={class_weight_ratio:.2f}")

    try:
        # ===== æ„å»ºæ•°æ®é›† =====
        train_dataset = ASVRawDataset(data_path, 'train', data_path / 'train.txt')
        dev_dataset = ASVRawDataset(data_path, 'dev', data_path / 'dev.txt')
        
        train_loader = torch.utils.data.DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True, 
            num_workers=4, 
            pin_memory=True,
            drop_last=True
        )
        dev_loader = torch.utils.data.DataLoader(
            dev_dataset, 
            batch_size=batch_size, 
            shuffle=False, 
            num_workers=4, 
            pin_memory=True,
            drop_last=True
        )

        # ===== æ„å»ºæ¨¡å‹ =====
        genotype = eval(arch)
        
        # åˆ›å»ºargså¯¹è±¡ (æ¨¡æ‹ŸåŸå§‹ä»£ç ä¸­çš„args)
        class Args:
            def __init__(self):
                self.nfft = 1024
                self.hop = 4
                self.nfilter = 70
                self.num_ceps = 20
                self.sr = 16000
                self.is_log = True
                self.is_mask = False
                self.is_cmvn = False
                self.report_freq = 1000
        
        args = Args()
        
        model = Network(16, 4, args, 2, genotype, 'LFCC').to(device)
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        ckpt = torch.load(pretrained_model, map_location='cpu')
        model.load_state_dict(ckpt.get('state_dict', ckpt), strict=False)
        
        # å†»ç»“backbone
        for name, param in model.named_parameters():
            if 'classifier' not in name.lower():
                param.requires_grad = False

        # ===== Loss & Optimizer =====
        weight = torch.FloatTensor([1.0, class_weight_ratio]).to(device)
        criterion = nn.CrossEntropyLoss(weight=weight)
        optimizer = torch.optim.AdamW(
            model.parameters(), 
            lr=lr, 
            weight_decay=weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        scheduler = ReduceLROnPlateau(
            optimizer, 
            mode='min', 
            factor=0.5, 
            patience=3, 
            verbose=False,
            min_lr=1e-6
        )

        # åˆ›å»ºwriter_dict (ç®€åŒ–ç‰ˆ)
        # åˆ›å»ºä¸€ä¸ªå‡çš„writerå¯¹è±¡æ¥é¿å…Noneé”™è¯¯
        class DummyWriter:
            def add_scalar(self, *args, **kwargs):
                pass
            def close(self):
                pass
        
        writer_dict = {
            'writer': DummyWriter(),  # ä½¿ç”¨å‡çš„writerå¯¹è±¡
            'train_global_steps': 0,
            'valid_global_steps': 0,
        }

        best_eer = float('inf')
        patience = 0
        max_patience = 5  # æå‰åœæ­¢

        for epoch in range(25):  # å‡å°‘epochæ•°ä»¥åŠ å¿«æœç´¢
            # è®¾ç½®drop_path_prob
            model.drop_path_prob = drop_path_prob * epoch / 25
            
            # è®­ç»ƒ
            train_acc, train_loss = train_from_scratch(
                args, train_loader, model, optimizer, criterion, epoch, writer_dict
            )
            
            # éªŒè¯
            dev_acc, dev_eer, dev_frr = validate(
                dev_loader, model, criterion, epoch, writer_dict, validate_type='dev'
            )

            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step(dev_eer)
            
            # è®°å½•æœ€ä½³ç»“æœ
            if dev_eer < best_eer:
                best_eer = dev_eer
                patience = 0
            else:
                patience += 1
            
            # æŠ¥å‘Šä¸­é—´ç»“æœç»™optuna
            trial.report(dev_eer, epoch)
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥å‰ªæ
            if trial.should_prune():
                raise optuna.exceptions.TrialPruned()
            
            # æ—©åœ
            if patience >= max_patience:
                print(f"Early stopping at epoch {epoch}, best EER: {best_eer:.4f}")
                break
            
            # æ¯5ä¸ªepochæ‰“å°ä¸€æ¬¡è¿›åº¦
            if epoch % 5 == 0:
                print(f"Epoch {epoch}: EER={dev_eer:.4f}, best_EER={best_eer:.4f}")

        print(f"âœ… Trial {trial.number} completed with best EER: {best_eer:.4f}")
        return best_eer

    except Exception as e:
        print(f"âŒ Trial {trial.number} failed: {str(e)}")
        raise optuna.exceptions.TrialPruned()

def run_optuna():
    print("ğŸ” Starting Optuna hyperparameter optimization...")
    
    # åˆ›å»ºç ”ç©¶
    study = optuna.create_study(
        direction="minimize",
        study_name="asv_finetune_optimization",
        storage="sqlite:///optuna_finetune.db",
        load_if_exists=True,
        pruner=optuna.pruners.MedianPruner(
            n_startup_trials=5,
            n_warmup_steps=5,
        ),
        sampler=optuna.samplers.TPESampler(seed=42)
    )
    
    # è¿è¡Œä¼˜åŒ–
    try:
        study.optimize(
            objective, 
            n_trials=20,  # å¯ä»¥æ ¹æ®æ—¶é—´è°ƒæ•´
            timeout=3600*4,  # 4å°æ—¶è¶…æ—¶
            show_progress_bar=True,
            callbacks=[
                lambda study, trial: print(f"ğŸ“Š Trials completed: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}")
                if len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]) > 0 
                else print(f"ğŸ“Š Trial {trial.number} completed")
            ]
        )
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Optimization interrupted by user")
    
    # è¾“å‡ºç»“æœ
    print("\n" + "="*50)
    print("ğŸ¯ OPTIMIZATION RESULTS")
    print("="*50)
    
    completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]
    
    if len(completed_trials) > 0:
        print(f"Best EER: {study.best_value:.4f}")
        print(f"Best parameters:")
        for key, value in study.best_params.items():
            print(f"  {key}: {value}")
    else:
        print("No trials completed successfully.")
    
    print(f"Total trials: {len(study.trials)}")
    print(f"Completed trials: {len(completed_trials)}")
    print(f"Pruned trials: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}")
    print(f"Failed trials: {len([t for t in study.trials if t.state == optuna.trial.TrialState.FAIL])}")
    
    # ä¿å­˜ç»“æœ
    import json
    if len(completed_trials) > 0:
        with open('best_params_finetune.json', 'w') as f:
            json.dump(study.best_params, f, indent=2)
        print(f"ğŸ’¾ Best parameters saved to best_params_finetune.json")
    else:
        print("âš ï¸ No best parameters to save.")
    
    # ä¿å­˜è¯•éªŒå†å²
    df = study.trials_dataframe()
    df.to_csv('optuna_finetune_trials.csv', index=False)
    print(f"ğŸ“‹ Trial history saved to optuna_finetune_trials.csv")

def plot_results():
    """ç»˜åˆ¶ä¼˜åŒ–ç»“æœ"""
    try:
        import optuna.visualization as vis
        
        study = optuna.load_study(
            study_name="asv_finetune_optimization",
            storage="sqlite:///optuna_finetune.db"
        )
        
        # ä¼˜åŒ–å†å²
        fig1 = vis.plot_optimization_history(study)
        fig1.write_html('finetune_optimization_history.html')
        print("ğŸ“ˆ Optimization history saved to finetune_optimization_history.html")
        
        # å‚æ•°é‡è¦æ€§
        fig2 = vis.plot_param_importances(study)
        fig2.write_html('finetune_param_importances.html')
        print("ğŸ“Š Parameter importances saved to finetune_param_importances.html")
        
    except ImportError:
        print("âš ï¸ Install plotly for visualization: pip install plotly")

if __name__ == "__main__":
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    import os
    if not os.path.exists('pre_trained_models/4-16.pth'):
        print("âŒ pre_trained_models/4-16.pth not found!")
        exit(1)
    
    if not os.path.exists('../../data/test_sample'):
        print("âŒ ../../data/test_sample not found!")
        exit(1)
    
    # è¿è¡Œä¼˜åŒ–
    run_optuna()
    
    # ç”Ÿæˆå¯è§†åŒ–
    plot_results()
    
    print("\nğŸ‰ Finetune optimization completed!")
    print("Next steps:")
    print("1. Check best_params_finetune.json for optimal hyperparameters")
    print("2. Run training with best parameters using your original finetune_v2.py")